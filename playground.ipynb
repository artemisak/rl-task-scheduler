{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium --quiet\n",
    "%pip install pettingzoo --quiet\n",
    "%pip install pygame --quiet\n",
    "%pip install numpy --quiet\n",
    "%pip install scipy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custon Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "from pettingzoo import ParallelEnv\n",
    "from scipy.stats import levy_stable\n",
    "\n",
    "\n",
    "class SurgeryQuotaScheduler(ParallelEnv):\n",
    "\n",
    "    metadata = {'render_modes': ['human', 'terminal'],\n",
    "                'name': 'sqsc_v1'}\n",
    "\n",
    "    def __init__(self, render_mode=None, max_agents=12, max_days=7, max_episode_length=7):\n",
    "        self.max_agents = max_agents\n",
    "        self.max_days = max_days\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.possible_agents = [\"agent_\" + str(r) for r in range(self.max_agents)]\n",
    "        self.agent_name_mapping = dict(zip(self.possible_agents, list(range(len(self.possible_agents)))))\n",
    "        self.agent_action_mapping = {0: 1, 1: -1, 2: 0}\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return MultiDiscrete([3, 2, 2, self.max_days, *[self.max_agents+2 for _ in range(self.max_days)]], start=[1, 0, 0, 0, *[-1 for _ in range(self.max_days)]])\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            win_width, win_height = 1000, 600\n",
    "            win = pygame.display.set_mode((win_width, win_height))\n",
    "            pygame.display.set_caption(\"Surgery Quota Scheduler\")\n",
    "            run = True\n",
    "            while run:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        run = False\n",
    "                win.fill((255, 255, 255))\n",
    "                font = pygame.font.Font(None, 36)\n",
    "                y = 50\n",
    "                for agent, pos in {agent: self.agents_data[agent]['position'] for agent in self.agents}.items():\n",
    "                    text = font.render(f\"{agent}: Day {pos + 1}\", True, (0, 0, 0))\n",
    "                    win.blit(text, (50, y))\n",
    "                    y += 30\n",
    "                pygame.display.flip()\n",
    "            pygame.quit()            \n",
    "        elif self.render_mode == 'terminal':\n",
    "            return self.observed_state\n",
    "        else:\n",
    "            gymnasium.logger.warn('You are calling render mode without specifying any render mode.')\n",
    "            return\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.quit()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.num_moves = 0\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.target_state = {day: (boundaries['min'] + boundaries['max']) / 2 for day, boundaries in options['target_state'].items()} if options is not None else {0: 3, 1: 2, 2: 2, 3: 2, 4: 1, 5: 1, 6: 1}\n",
    "        self.agents_data = options['agents_data'] if options is not None else {agent: {'active': True,\n",
    "                                                                                       'position': self.rng.integers(0, 7, 1).item(),\n",
    "                                                                                       'base_reward': 1.0,\n",
    "                                                                                       'window': 5,\n",
    "                                                                                       'alpha': 2.0,\n",
    "                                                                                       'urgency': self.rng.integers(1, 4, 1).item(),\n",
    "                                                                                       'completeness': self.rng.integers(0, 2, 1).item(),\n",
    "                                                                                       'complexity': self.rng.integers(0, 2, 1).item(),\n",
    "                                                                                       'mutation_rate': 0.0} for agent in self.agents}\n",
    "        observations = {agent: np.array([self.agents_data[agent]['urgency'],\n",
    "                                         self.agents_data[agent]['completeness'],\n",
    "                                         self.agents_data[agent]['complexity'],\n",
    "                                         self.agents_data[agent]['position'],\n",
    "                                         *[self.observed_state[day] if day in [int((self.agents_data[agent]['position'] + np.ceil(d - self.agents_data[agent]['window'] / 2)) % self.max_days) for d in range(self.agents_data[agent]['window'])] else -1 for day in self.observed_state]])\n",
    "                                         for agent in self.agents}\n",
    "        infos = {agent: self.agents_data[agent] for agent in self.agents}\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        if not actions:\n",
    "            return {}, {}, {}, {}, {}\n",
    "        self.num_moves += 1\n",
    "        for agent in self.agents:\n",
    "            if self.agents_data[agent]['active']:\n",
    "                self.agents_data[agent]['position'] = (self.agents_data[agent]['position'] + self.agent_action_mapping[int(actions[agent])]) % self.max_days\n",
    "                if self.agents_data[agent]['position'] > (self.max_days // 2):\n",
    "                    if int(actions[agent]) == 0:\n",
    "                        if self.agents_data[agent]['mutation_rate'] != 1.0:\n",
    "                            self.agents_data[agent]['mutation_rate'] = min(self.agents_data[agent]['mutation_rate'] + 0.05, 1.0)\n",
    "                    elif int(actions[agent]) == 1:\n",
    "                        if self.agents_data[agent]['mutation_rate'] != 0.0:\n",
    "                            self.agents_data[agent]['mutation_rate'] = max(self.agents_data[agent]['mutation_rate'] - 0.05, 0.0)\n",
    "        rewards = {agent: self.reward_map(agent) for agent in self.agents}\n",
    "        terminations = {agent: list(actions.values()).count(2) > self.max_agents * 0.8 for agent in self.agents}\n",
    "        truncations = {agent: self.num_moves >= self.max_episode_length for agent in self.agents}\n",
    "        if any(terminations.values()) or any(truncations.values()):\n",
    "            rewards = {agent: r - 0.5 * abs(self.observed_state[self.agents_data[agent]['position']] - self.target_state[self.agents_data[agent]['position']]) for agent, r in rewards.items()}\n",
    "        observations = {agent: np.array([self.agents_data[agent]['urgency'],\n",
    "                                         self.agents_data[agent]['completeness'],\n",
    "                                         self.agents_data[agent]['complexity'],\n",
    "                                         self.agents_data[agent]['position'],\n",
    "                                         *[self.observed_state[day] if day in [int((self.agents_data[agent]['position'] + np.ceil(d - self.agents_data[agent]['window'] / 2)) % self.max_days) for d in range(self.agents_data[agent]['window'])] else -1 for day in self.observed_state]])\n",
    "                                         for agent in self.agents}\n",
    "        infos = {agent: self.agents_data[agent] for agent in self.agents}\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    @property\n",
    "    def observed_state(self):\n",
    "        return {day: [self.agents_data[agent]['position'] for agent in self.agents].count(day) for day in range(self.max_days)}\n",
    "    \n",
    "    def reward_map(self, agent):\n",
    "        discrepancy = {day: abs(self.observed_state[day] - self.target_state[day]) for day in self.target_state}\n",
    "        window = [int((self.agents_data[agent]['position'] + np.ceil(\n",
    "            d - self.agents_data[agent]['window'] / 2)) % self.max_days) for d in\n",
    "                range(self.agents_data[agent]['window'])]\n",
    "        masked_discrepancy = {day: discrepancy[day] if day in window else 0 for day in discrepancy}\n",
    "        rv = levy_stable(self.agents_data[agent]['alpha'], 0.0, loc=self.agents_data[agent]['position'], scale=1.0)\n",
    "        weighted_discrepancy_sum = np.sum([rv.pdf(day) * masked_discrepancy[day] for day in masked_discrepancy])\n",
    "\n",
    "        global_discrepancy = sum(discrepancy.values())\n",
    "\n",
    "        reward = - (weighted_discrepancy_sum + 0.05 * global_discrepancy) * self.agents_data[agent]['base_reward'] / 2\n",
    "\n",
    "        current_day = self.agents_data[agent]['position']\n",
    "        if self.observed_state[current_day] < self.target_state[current_day]:\n",
    "            reward += 0.5 * self.agents_data[agent]['base_reward']\n",
    "\n",
    "        if self.observed_state[current_day] > self.target_state[current_day]:\n",
    "            reward -= 0.1 * self.agents_data[agent]['base_reward']\n",
    "\n",
    "        scale = max(1, (self.agents_data[agent]['complexity'] + (1 - self.agents_data[agent]['completeness'])) * self.agents_data[agent]['urgency'])\n",
    "        if scale > 3:\n",
    "            reward -= (self.agents_data[agent]['position'] - 1) / 5\n",
    "\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch --quiet\n",
    "%pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, obs_dim, action_dim, lr_actor=3e-4, lr_critic=3e-4, gamma=0.99, epsilon=0.2):\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "        self.critic = Critic(obs_dim)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs)\n",
    "        probs = self.actor(obs)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "    \n",
    "    def update(self, obs, actions, old_log_probs, rewards, next_obs, dones):\n",
    "        obs = torch.FloatTensor(obs)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_obs = torch.FloatTensor(next_obs)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        values = self.critic(obs).squeeze()\n",
    "        next_values = self.critic(next_obs).squeeze()\n",
    "        td_target = rewards + self.gamma * next_values * (1 - dones)\n",
    "        td_error = td_target - values\n",
    "        advantage = td_error.detach()\n",
    "        \n",
    "        critic_loss = (td_error ** 2).mean()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        probs = self.actor(obs)\n",
    "        dist = Categorical(probs)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "\n",
    "class MAPPOTrainer:\n",
    "    def __init__(self, env, n_agents, obs_dim, action_dim, lr_actor=3e-4, lr_critic=3e-4, gamma=0.99, epsilon=0.2):\n",
    "        self.env = env\n",
    "        self.n_agents = n_agents\n",
    "        self.agents = [MAPPOAgent(obs_dim, action_dim, lr_actor, lr_critic, gamma, epsilon) for _ in range(n_agents)]\n",
    "        self.writer = SummaryWriter()\n",
    "    \n",
    "    def train(self, n_episodes, max_steps, log_interval=5):\n",
    "        pbar = tqdm(total=n_episodes, desc=\"Training Progress\")\n",
    "        for episode in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            episode_rewards = [0 for _ in range(self.n_agents)]\n",
    "            episode_critic_losses = []\n",
    "            episode_actor_losses = []\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                actions = []\n",
    "                old_log_probs = []\n",
    "                \n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    action, log_prob = agent.get_action(obs[f\"agent_{i}\"])\n",
    "                    actions.append(action)\n",
    "                    old_log_probs.append(log_prob)\n",
    "                \n",
    "                next_obs, rewards, dones, truncations, _ = self.env.step({f\"agent_{i}\": a for i, a in enumerate(actions)})\n",
    "                \n",
    "                for i in range(self.n_agents):\n",
    "                    episode_rewards[i] += rewards[f\"agent_{i}\"]\n",
    "                \n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    critic_loss, actor_loss = agent.update(\n",
    "                        obs[f\"agent_{i}\"].reshape(1, -1),\n",
    "                        [actions[i]],\n",
    "                        [old_log_probs[i].item()],\n",
    "                        [rewards[f\"agent_{i}\"]],\n",
    "                        next_obs[f\"agent_{i}\"].reshape(1, -1),\n",
    "                        [dones[f\"agent_{i}\"] or truncations[f\"agent_{i}\"]]\n",
    "                    )\n",
    "                    episode_critic_losses.append(critic_loss)\n",
    "                    episode_actor_losses.append(actor_loss)\n",
    "                \n",
    "                obs = next_obs\n",
    "                \n",
    "                if all(dones.values()) or all(truncations.values()):\n",
    "                    break\n",
    "            \n",
    "            if (episode + 1) % log_interval == 0:\n",
    "                avg_reward = sum(episode_rewards) / self.n_agents\n",
    "                avg_critic_loss = np.mean(episode_critic_losses)\n",
    "                avg_actor_loss = np.mean(episode_actor_losses)\n",
    "                self.writer.add_scalar('Reward/average', avg_reward, episode)\n",
    "                self.writer.add_scalar('Loss/critic', avg_critic_loss, episode)\n",
    "                self.writer.add_scalar('Loss/actor', avg_actor_loss, episode)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'avg_reward': f'{sum(episode_rewards) / self.n_agents:.2f}'})\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            torch.save(agent.actor.state_dict(), os.path.join(path, f'actor_{i}.pth'))\n",
    "            torch.save(agent.critic.state_dict(), os.path.join(path, f'critic_{i}.pth'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = SurgeryQuotaScheduler()\n",
    "    n_agents = 12\n",
    "    obs_dim = env.observation_space(\"agent_0\").shape[0]\n",
    "    action_dim = env.action_space(\"agent_0\").n\n",
    "    \n",
    "    trainer = MAPPOTrainer(env, n_agents, obs_dim, action_dim)\n",
    "    trainer.train(n_episodes=12000, max_steps=7, log_interval=10)\n",
    "    trainer.save_model('trained_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPPO tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.actor.load_state_dict(torch.load(path))\n",
    "        self.actor.eval()\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs)\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(obs)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "class MAPPOTester:\n",
    "\n",
    "    def __init__(self, env, n_agents, obs_dim, action_dim, options=None):\n",
    "        self.env = env\n",
    "        self.options = options\n",
    "        self.n_agents = n_agents\n",
    "        self.agents = [MAPPOAgent(obs_dim, action_dim) for _ in range(n_agents)]\n",
    "\n",
    "    def calculate_deviation(self, observed_states, target_state):\n",
    "        num_episodes = len(observed_states)\n",
    "        bootstrap_average_distribution = {i: sum(episode[i] for episode in observed_states) / num_episodes for i in\n",
    "                                          range(7)}\n",
    "\n",
    "        bootstrap_average_percentage_deviations = {i: 0 for i in range(7)}\n",
    "        mean_target_state = {day: (target_state[day]['max'] + target_state[day]['min']) / 2 for day in target_state}\n",
    "        for key, value in bootstrap_average_distribution.items():\n",
    "            bootstrap_average_percentage_deviations[key] = np.abs(mean_target_state[key] - value) / mean_target_state[key]\n",
    "        \n",
    "        average_bootstrap_deviation = np.mean(list(bootstrap_average_percentage_deviations.values()))\n",
    "        std_bootstrap_deviation = np.std(list(bootstrap_average_percentage_deviations.values()))\n",
    "        return average_bootstrap_deviation, std_bootstrap_deviation\n",
    "\n",
    "    def test(self, n_episodes, max_steps, target_state):\n",
    "        all_observed_states = []\n",
    "        all_final_positions = []\n",
    "        all_scaling_factors = []\n",
    "\n",
    "        for e in tqdm(range(n_episodes), desc=\"Bootstrap testing...\"):\n",
    "            obs, info = self.env.reset(options=self.options)\n",
    "\n",
    "            # print(f'Episode {e}:')\n",
    "            # print(f'Initial environment state', self.env.render())\n",
    "            # print('Beliefs: ', obs)\n",
    "            episode_observed_states = []\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                actions = {}\n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    action = agent.get_action(obs[f\"agent_{i}\"])\n",
    "                    actions[f\"agent_{i}\"] = action\n",
    "                # print('Intentions: ', actions)\n",
    "                next_obs, rewards, dones, truncations, info = self.env.step(actions)\n",
    "                # print('Desires: ', rewards)\n",
    "                # print(f'Environment state on step {step}:', self.env.render())\n",
    "                # print('Beliefs: ', next_obs)\n",
    "                \n",
    "                if any(dones.values()) or any(truncations.values()):\n",
    "                    episode_observed_states.append(self.env.observed_state)\n",
    "                    all_final_positions.extend([agent_info['position'] for agent_info in info.values()])\n",
    "                    all_scaling_factors.extend([max(1, (agent_info['complexity'] + (1 - agent_info['completeness'])) * agent_info['urgency'])  for agent_info in info.values()])\n",
    "                    # all_scaling_factors.extend([agent_info['scaling_factor'] for agent_info in info.values()])\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "                if all(dones.values()) or all(truncations.values()):\n",
    "                    break\n",
    "\n",
    "            all_observed_states.extend(episode_observed_states)\n",
    "\n",
    "        mean_deviation, std_deviation = self.calculate_deviation(all_observed_states, target_state)\n",
    "        avg_bids_per_day = {day: sum(state[day] for state in all_observed_states) / len(all_observed_states) for day in\n",
    "                            range(7)}\n",
    "\n",
    "        scaling_factor_positions = {}\n",
    "        for sf, pos in zip(all_scaling_factors, all_final_positions):\n",
    "            if sf not in scaling_factor_positions:\n",
    "                scaling_factor_positions[sf] = []\n",
    "            scaling_factor_positions[sf].append(pos)\n",
    "        avg_position_per_scaling_factor = {sf: np.mean(positions) for sf, positions in scaling_factor_positions.items()}\n",
    "\n",
    "        return mean_deviation, std_deviation, avg_bids_per_day, avg_position_per_scaling_factor\n",
    "\n",
    "    def bootstrap_test(self, n_episodes, max_steps, target_state):\n",
    "        mean_deviation, std_deviation, avg_bids, avg_positions = self.test(n_episodes, max_steps, target_state)\n",
    "\n",
    "        print(f\"Operator preferences: {target_state}\")\n",
    "        print(f\"Average number of bids per day: {avg_bids}\")\n",
    "        print(f\"Percentage deviation: mean = {mean_deviation:.2%}, std = {std_deviation: .2%}\")\n",
    "        print(f\"Average position per scaling factor: {avg_positions}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent.load_model(os.path.join(path, f'actor_{i}.pth'))\n",
    "        print(f\"Loaded model from {path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = SurgeryQuotaScheduler(render_mode='terminal')\n",
    "    n_agents = 12\n",
    "    obs_dim = env.observation_space(\"agent_0\").shape[0]\n",
    "    action_dim = env.action_space(\"agent_0\").n\n",
    "    \n",
    "    tester = MAPPOTester(env, n_agents, obs_dim, action_dim)\n",
    "    tester.load_model(path='trained_model')\n",
    "    tester.bootstrap_test(n_episodes=10000, max_steps=7,\n",
    "                          target_state={0: {'min': 3, 'max': 3},\n",
    "                                        1: {'min': 2, 'max': 2},\n",
    "                                        2: {'min': 2, 'max': 2},\n",
    "                                        3: {'min': 2, 'max': 2},\n",
    "                                        4: {'min': 1, 'max': 1},\n",
    "                                        5: {'min': 1, 'max': 1},\n",
    "                                        6: {'min': 1, 'max': 1}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.actor.load_state_dict(torch.load(path, weights_only=True))\n",
    "        self.actor.eval()\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs)\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(obs)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, name, urgency, completeness, complexity) -> None:\n",
    "        self.name = name\n",
    "        self.urgency = urgency\n",
    "        self.completeness = completeness\n",
    "        self.complexity = complexity\n",
    "        self.acceptance_rate = np.random.randint(50, 76) / 100\n",
    "        self._day = np.random.randint(0, 7)\n",
    "        self._satisfied = False\n",
    "        self._assigned_agent = None\n",
    "\n",
    "    @property\n",
    "    def appointment_day(self):\n",
    "        return self._day\n",
    "\n",
    "    @appointment_day.setter\n",
    "    def appointment_day(self, value):\n",
    "        self._day = value\n",
    "\n",
    "    @property\n",
    "    def satisfied(self):\n",
    "        return self._satisfied\n",
    "\n",
    "    @satisfied.setter\n",
    "    def satisfied(self, value):\n",
    "        self._satisfied = value\n",
    "\n",
    "    @property\n",
    "    def assigned_agent(self):\n",
    "        return self._assigned_agent\n",
    "\n",
    "    @assigned_agent.setter\n",
    "    def assigned_agent(self, value):\n",
    "        self._assigned_agent = value\n",
    "\n",
    "    def give_feedback(self):\n",
    "        answer = np.random.choice([True, False], p=[self.acceptance_rate, 1 - self.acceptance_rate])\n",
    "        if answer == True: \n",
    "            self.acceptance_rate = 1.0\n",
    "        return answer\n",
    "    \n",
    "\n",
    "class MultiAgentSystemOperator:\n",
    "    def __init__(self, list_of_clients) -> None:\n",
    "        self.clients = list_of_clients\n",
    "    \n",
    "    def collect_feedback(self):\n",
    "        for client in self.clients:\n",
    "            client.satisfied = client.give_feedback()\n",
    "    \n",
    "    def assign_agnets(self, agents):\n",
    "        for client in self.clients:\n",
    "            health_state = (client.urgency, client.completeness, client.complexity)\n",
    "            client.assigned_agent = agents[health_state]\n",
    "\n",
    "    def get_actions(self, observaions):\n",
    "        actions = {}\n",
    "        for agent, client in zip(observations, self.clients):\n",
    "            model = client.assigned_agent['model_file']      \n",
    "            actions[agent] = model.get_action(observaions[agent])\n",
    "        return actions\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        handlers=[\n",
    "                            logging.FileHandler(\"logfile.log\"),\n",
    "                            logging.StreamHandler()\n",
    "                        ])\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "\n",
    "    urgency = range(1, 4)\n",
    "    completeness = range(0, 2)\n",
    "    complexity = range(0, 2)\n",
    "\n",
    "    options = list(itertools.product(urgency, completeness, complexity))\n",
    "\n",
    "    selected_states = np.random.choice(len(options), size=12, replace=False)\n",
    "    health_states  = [options[i] for i in selected_states]\n",
    "\n",
    "    patiens = [Client(name=f'client_{i}',\n",
    "                      urgency=np.random.randint(1, 4),\n",
    "                      completeness=np.random.randint(0, 2), \n",
    "                      complexity=np.random.randint(0, 2)) for i in range(100)]\n",
    "\n",
    "    agents = {f'agent_{i}': MAPPOAgent(obs_dim=11, action_dim=3) for i in range(len(health_states))}\n",
    "\n",
    "    for i, agent in enumerate(agents):\n",
    "        agents[agent].load_model(f'trained_model/actor_{i}.pth')\n",
    "\n",
    "    manager = MultiAgentSystemOperator(list_of_clients=patiens)\n",
    "    manager.assign_agnets({health_state: {'agent_name': agent_name, 'model_file': model_file} for health_state, (agent_name, model_file) in zip(health_states, agents.items())})\n",
    "\n",
    "    e = 0\n",
    "\n",
    "    while not all([client.satisfied for client in manager.clients]):\n",
    "\n",
    "        logger.info(f\"Starting episode {e}\")\n",
    "\n",
    "        env = SurgeryQuotaScheduler(render_mode='terminal', max_agents=len(patiens),\n",
    "                                    max_days=7, max_episode_length=7)\n",
    "        observations, _ = env.reset(\n",
    "            options={\n",
    "                'target_state': {0: {'min': 3, 'max': 3},\n",
    "                                1: {'min': 2, 'max': 2},\n",
    "                                2: {'min': 2, 'max': 2},\n",
    "                                3: {'min': 2, 'max': 2},\n",
    "                                4: {'min': 1, 'max': 1},\n",
    "                                5: {'min': 1, 'max': 1},\n",
    "                                6: {'min': 1, 'max': 1}},\n",
    "                'agents_data': {f'agent_{i}': {'active': ~client.satisfied,\n",
    "                                               'position': client.appointment_day if client.satisfied else np.random.randint(0, 7),\n",
    "                                               'base_reward': 1.0,\n",
    "                                               'window': 5,\n",
    "                                               'alpha': 2.0,\n",
    "                                               'urgency': client.urgency,\n",
    "                                               'completeness': client.completeness,\n",
    "                                               'complexity': client.complexity,\n",
    "                                               'mutation_rate': 0.0}\n",
    "                                for i, client in enumerate(manager.clients)\n",
    "                                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Episode {e} - {env.render()}\")\n",
    "\n",
    "        while True:\n",
    "            actions = manager.get_actions(observaions=observations)\n",
    "            logger.debug(f\"Actions: {actions}\")\n",
    "            observations, _, dones, truncations, _ = env.step(actions)\n",
    "\n",
    "            for agent, client in zip(observations, manager.clients):\n",
    "                 client.appointment_day = observations[agent][3]\n",
    "\n",
    "            logger.info(f\"Episode {e} - {env.render()}\")\n",
    "\n",
    "            if any(dones.values()) or any(truncations.values()):\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        manager.collect_feedback()\n",
    "\n",
    "        e += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow --quiet\n",
    "%pip install tensorboard --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Comparison: Naive Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import levy_stable\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "class GeneticAlgorithmScheduler:\n",
    "    def __init__(self, max_agents=12, max_days=7, population_size=100, generations=1000):\n",
    "        self.max_agents = max_agents\n",
    "        self.max_days = max_days\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.agents_data = self.generate_agents_data()\n",
    "        self.target_state = {day: self.max_agents / self.max_days for day in range(self.max_days)}\n",
    "        \n",
    "        self.log_dir = 'logs/genetic_algorithm'\n",
    "        self.summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    def generate_agents_data(self):\n",
    "        agents_data = {}\n",
    "        rng = np.random.default_rng()\n",
    "        for (urgency, complexity, completeness), agent in zip(\n",
    "            itertools.product(range(1, 4), range(0, 2), range(0, 2)),\n",
    "            [f\"agent_{r}\" for r in range(self.max_agents)]\n",
    "        ):\n",
    "            agents_data[agent] = {\n",
    "                'active': True,\n",
    "                'base_reward': 1.0,\n",
    "                'window': 3,\n",
    "                'alpha': 2.0,\n",
    "                'urgency': urgency,\n",
    "                'complexity': complexity,\n",
    "                'completeness': completeness,\n",
    "                'mutation_rate': 0.0,\n",
    "                'position': rng.integers(0, 7, 1).item(),\n",
    "                'scaling_factor': ((complexity + (1 - completeness)) * urgency)\n",
    "            }\n",
    "        return agents_data\n",
    "\n",
    "    def create_individual(self):\n",
    "        return np.array([self.agents_data[f\"agent_{i}\"][\"position\"] for i in range(self.max_agents)])\n",
    "\n",
    "    def create_population(self):\n",
    "        return [self.create_individual() for _ in range(self.population_size)]\n",
    "\n",
    "    def fitness(self, individual):\n",
    "        observed_state = {day: 0 for day in range(self.max_days)}\n",
    "        for agent, day in enumerate(individual):\n",
    "            observed_state[day] += 1\n",
    "        \n",
    "        fitness_score = 0\n",
    "        for agent, day in enumerate(individual):\n",
    "            agent_data = self.agents_data[f\"agent_{agent}\"]\n",
    "            discrepancy = {d: abs(observed_state[d] - self.target_state[d]) for d in range(self.max_days)}\n",
    "            window = [int((day + np.ceil(d - agent_data['window'] / 2)) % self.max_days) for d in range(agent_data['window'])]\n",
    "            masked_discrepancy = {d: discrepancy[d] if d in window else 0 for d in discrepancy}\n",
    "            rv = levy_stable(agent_data['alpha'], 0.0, loc=day, scale=1.0)\n",
    "            weighted_discrepancy_sum = np.sum([rv.pdf(d) * masked_discrepancy[d] for d in masked_discrepancy])\n",
    "            reward = -weighted_discrepancy_sum * agent_data['base_reward']\n",
    "            fitness_score += reward\n",
    "        \n",
    "        return fitness_score\n",
    "\n",
    "    def selection(self, population, k=2):\n",
    "        selected = np.random.choice(len(population), k, replace=False)\n",
    "        return max([population[i] for i in selected], key=self.fitness)\n",
    "\n",
    "    def crossover(self, parent1, parent2):\n",
    "        crossover_point = np.random.randint(1, len(parent1))\n",
    "        child = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
    "        return child\n",
    "\n",
    "    def mutation(self, individual, mutation_rate=0.01):\n",
    "        for i in range(len(individual)):\n",
    "            if np.random.random() < mutation_rate:\n",
    "                individual[i] = np.random.randint(0, self.max_days)\n",
    "        return individual\n",
    "\n",
    "    def run(self):\n",
    "        population = self.create_population()\n",
    "        best_fitness_history = []\n",
    "        \n",
    "        for generation in tqdm(range(self.generations), leave=False):\n",
    "            new_population = []\n",
    "            \n",
    "            for _ in range(self.population_size):\n",
    "                parent1 = self.selection(population)\n",
    "                parent2 = self.selection(population)\n",
    "                child = self.crossover(parent1, parent2)\n",
    "                child = self.mutation(child)\n",
    "                new_population.append(child)\n",
    "            \n",
    "            population = new_population\n",
    "            \n",
    "            best_individual = max(population, key=self.fitness)\n",
    "            best_fitness = self.fitness(best_individual)\n",
    "            best_fitness_history.append(best_fitness)\n",
    "            \n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('Best Fitness', best_fitness, step=generation)\n",
    "        \n",
    "        return max(population, key=self.fitness), best_fitness_history\n",
    "\n",
    "    def get_schedule_details(self, best_individual):\n",
    "        schedule_details = []\n",
    "        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        \n",
    "        for agent, day in enumerate(best_individual):\n",
    "            agent_data = self.agents_data[f\"agent_{agent}\"]\n",
    "            schedule_details.append({\n",
    "                'agent': f\"agent_{agent}\",\n",
    "                'day': days[day],\n",
    "                'urgency': agent_data['urgency'],\n",
    "                'completeness': agent_data['completeness'],\n",
    "                'complexity': agent_data['complexity'],\n",
    "                'scaling_factor': agent_data['scaling_factor']\n",
    "            })\n",
    "        \n",
    "        return schedule_details\n",
    "\n",
    "def run_experiment(num_runs=5):\n",
    "    all_results = []\n",
    "    all_fitness_histories = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\nRun {run + 1}/{num_runs}\")\n",
    "        scheduler = GeneticAlgorithmScheduler()\n",
    "        best_schedule, fitness_history = scheduler.run()\n",
    "        schedule_details = scheduler.get_schedule_details(best_schedule)\n",
    "        all_results.append(schedule_details)\n",
    "        all_fitness_histories.append(fitness_history)\n",
    "\n",
    "        print(f\"Best fitness for run {run + 1}: {fitness_history[-1]}\")\n",
    "        for item in schedule_details:\n",
    "            print(f\"{item['agent']} placed on {item['day']}, urgency: {item['urgency']}, \"\n",
    "                  f\"completeness: {item['completeness']}, complexity: {item['complexity']}, \"\n",
    "                  f\"scaling_factor: {item['scaling_factor']}\")\n",
    "\n",
    "    return all_results, all_fitness_histories\n",
    "\n",
    "def summarize_results(all_results, all_fitness_histories):\n",
    "    num_runs = len(all_results)\n",
    "    \n",
    "    final_fitness_scores = [history[-1] for history in all_fitness_histories]\n",
    "    avg_final_fitness = np.mean(final_fitness_scores)\n",
    "    std_final_fitness = np.std(final_fitness_scores)\n",
    "    \n",
    "    print(f\"\\nSummary across {num_runs} runs:\")\n",
    "    print(f\"Average final fitness: {avg_final_fitness:.2f} Â± {std_final_fitness:.2f}\")\n",
    "\n",
    "    day_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for run_results in all_results:\n",
    "        for item in run_results:\n",
    "            day_counts[item['agent']][item['day']] += 1\n",
    "    \n",
    "    print(\"\\nMost common day assignments:\")\n",
    "    for agent in day_counts:\n",
    "        most_common_day = max(day_counts[agent], key=day_counts[agent].get)\n",
    "        frequency = day_counts[agent][most_common_day]\n",
    "        print(f\"{agent}: {most_common_day} ({frequency}/{num_runs} runs)\")\n",
    "\n",
    "    convergence_points = []\n",
    "    for history in all_fitness_histories:\n",
    "        for i, fitness in enumerate(history):\n",
    "            if i > 0 and abs(fitness - history[-1]) < 0.01 * abs(history[-1]):\n",
    "                convergence_points.append(i)\n",
    "                break\n",
    "    avg_convergence = np.mean(convergence_points)\n",
    "    print(f\"\\nAverage convergence generation: {avg_convergence:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_results, all_fitness_histories = run_experiment(num_runs=5)\n",
    "    summarize_results(all_results, all_fitness_histories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comparison: Expert Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, name, urgency, completeness, complexity):\n",
    "        self.name = name\n",
    "        self.urgency = urgency\n",
    "        self.completeness = completeness\n",
    "        self.complexity = complexity\n",
    "\n",
    "    @property\n",
    "    def scaling_factor(self):\n",
    "        return int(round((self.complexity + (1 - self.completeness)) * self.urgency))\n",
    "\n",
    "class Manager:\n",
    "    def __init__(self, planning_horizon, preferences):\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.preferences = preferences if preferences is not None else {\n",
    "            day: {'min': np.min(numbers), 'max': np.max(numbers)}\n",
    "            for day, numbers in enumerate(np.random.randint(1, 3, (7, 2)))\n",
    "        }\n",
    "\n",
    "    def reset_schedule(self):\n",
    "        self.schedule = {day: [] for day in range(self.planning_horizon)}\n",
    "\n",
    "    def manage(self, upcoming_requests):\n",
    "        for request in upcoming_requests:\n",
    "            for day in range(self.planning_horizon):\n",
    "                if len(self.schedule[day]) < self.preferences[day]['max']:\n",
    "                    self.schedule[day].append(request)\n",
    "                    break\n",
    "        return self.schedule\n",
    "\n",
    "class Estimator:\n",
    "    def __init__(self, schedules, target_state=None):\n",
    "        self.observed_states = [{day: len(schedule[day]) for day in schedule.keys()} for schedule in schedules]\n",
    "        self.schedules = schedules\n",
    "        if target_state is not None:\n",
    "            self.target_state = target_state\n",
    "        else:\n",
    "            self.target_state = {\n",
    "                day: {'min': np.min(numbers), 'max': np.max(numbers)}\n",
    "                for day, numbers in enumerate(np.random.randint(1, 3, (7, 2)))\n",
    "            }\n",
    "\n",
    "    def describe(self):\n",
    "        num_episodes = len(self.observed_states)\n",
    "\n",
    "        deviations = []\n",
    "        total_clients_per_day = {i: 0 for i in range(len(self.schedules[0]))}\n",
    "        all_scaling_factors = []\n",
    "        all_final_positions = []\n",
    "\n",
    "        for schedule in self.schedules:\n",
    "            for day in schedule:\n",
    "                actual_clients = len(schedule[day])\n",
    "                total_clients_per_day[day] += actual_clients\n",
    "                target_min = self.target_state[day]['min']\n",
    "                target_max = self.target_state[day]['max']\n",
    "\n",
    "                deviation = max(0, target_min - actual_clients) + max(0, actual_clients - target_max)\n",
    "                deviations.append(deviation)\n",
    "\n",
    "                for client in schedule[day]:\n",
    "                    all_scaling_factors.append(client.scaling_factor)\n",
    "                    all_final_positions.append(day)\n",
    "\n",
    "        mean_deviation = np.mean(deviations) / num_episodes\n",
    "        std_deviation = np.std(deviations) / num_episodes\n",
    "        avg_clients_per_day = {day: total / num_episodes for day, total in total_clients_per_day.items()}\n",
    "\n",
    "        scaling_factor_positions = {}\n",
    "        for sf, pos in zip(all_scaling_factors, all_final_positions):\n",
    "            if sf not in scaling_factor_positions:\n",
    "                scaling_factor_positions[sf] = []\n",
    "            scaling_factor_positions[sf].append(pos)\n",
    "        avg_position_per_scaling_factor = {sf: np.mean(positions) for sf, positions in scaling_factor_positions.items()}\n",
    "\n",
    "        print(f\"Operator preferences: {self.target_state}\")\n",
    "        print(f\"Average number of bids per day: {avg_clients_per_day}\")\n",
    "        print(f\"Percentage deviation: mean = {mean_deviation:.2%}, std = {std_deviation: .2%}\")\n",
    "        print(f\"Average position per scaling factor: {avg_position_per_scaling_factor}\")\n",
    "\n",
    "def run_simulation(n_episodes=1):\n",
    "    planning_horizon = 7\n",
    "    health_state = list(itertools.product(range(1, 4),\n",
    "                                     range(0, 2),\n",
    "                                     range(0, 2)))\n",
    "\n",
    "    target_state = {\n",
    "        0: {'min': 3, 'max': 3},\n",
    "        1: {'min': 2, 'max': 2},\n",
    "        2: {'min': 2, 'max': 2},\n",
    "        3: {'min': 2, 'max': 2},\n",
    "        4: {'min': 1, 'max': 1},\n",
    "        5: {'min': 1, 'max': 1},\n",
    "        6: {'min': 1, 'max': 1},\n",
    "    }\n",
    "\n",
    "    manager = Manager(planning_horizon, preferences=target_state)\n",
    "    \n",
    "    schedules = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        manager.reset_schedule()\n",
    "        clients = [Client(name=f'client_{i}', urgency=urgency, completeness=completeness, complexity=complexity)\n",
    "                   for i, (urgency, completeness, complexity) in enumerate(health_state)]\n",
    "        schedules.append(manager.manage(np.random.choice(clients, size=12, replace=True)))\n",
    "\n",
    "    estimator = Estimator(schedules, target_state)\n",
    "    estimator.describe()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_simulation()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
